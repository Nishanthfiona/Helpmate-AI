{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "# Suppress Warnings"
      ],
      "metadata": {
        "id": "veNghWnXHfrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries\n",
        "!pip install -U -q langchain langchain-google-genai chromadb pdfplumber langchain-community sentence-transformers google-colab pypdf\n"
      ],
      "metadata": {
        "id": "LFtV8OXiC2r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries and Mount Drive\n",
        "print(\"--- Importing Libraries ---\")\n",
        "import os\n",
        "import getpass\n",
        "from pathlib import Path\n",
        "from google.colab import drive, userdata # For API Key Secret and Drive\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader # Using standard PDF loader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from sentence_transformers.cross_encoder import CrossEncoder # For reranker\n",
        "\n",
        "print(\"Libraries imported.\")\n",
        "\n",
        "try:\n",
        "    print(\"\\n--- Mounting Google Drive ---\")\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}. Ensure data paths are accessible.\")"
      ],
      "metadata": {
        "id": "v380HlKIC40W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Configure API Key and Paths\n",
        "print(\"--- Configuring API Key and Paths ---\")\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "     print(\"\\nError: Gemini API Key is not set. Further steps requiring the API will fail.\")\n",
        "else:\n",
        "\n",
        "     os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "     print(\"\\nGemini API Key is configured.\")\n",
        "\n",
        "pdf_directory_path = \"/content/drive/MyDrive/HelpMate AI Codes\"\n",
        "print(f\"PDF directory path set to: {pdf_directory_path}\")\n",
        "\n",
        "chroma_persist_path = \"/content/drive/MyDrive/HelpMate AI Codes/chroma_db_langchain\"\n",
        "print(f\"ChromaDB persistent path set to: {chroma_persist_path}\")\n",
        "\n",
        "# Ensure ChromaDB directory exists\n",
        "try:\n",
        "    os.makedirs(chroma_persist_path, exist_ok=True)\n",
        "    print(f\"Directory '{chroma_persist_path}' ensured.\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not create directory {chroma_persist_path}: {e}\")"
      ],
      "metadata": {
        "id": "F9IqVNCLC6qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Documents\n",
        "\n",
        "print(f\"\\n--- Loading Documents from {pdf_directory_path} using PyPDFDirectoryLoader ---\")\n",
        "# Import the necessary loader\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from pathlib import Path\n",
        "\n",
        "pdf_directory = Path(pdf_directory_path)\n",
        "docs = []\n",
        "\n",
        "if not pdf_directory.is_dir():\n",
        "    print(f\"Error: Directory not found at {pdf_directory_path}\")\n",
        "else:\n",
        "    try:\n",
        "        # PyPDFDirectoryLoader handles iterating through the directory\n",
        "        loader = PyPDFDirectoryLoader(\n",
        "            path=pdf_directory_path,\n",
        "            recursive=False # True if PDFs in subfolders\n",
        "            )\n",
        "        # Load documents - this loads all pages from all PDFs found\n",
        "        docs = loader.load()\n",
        "\n",
        "        if docs:\n",
        "            print(f\"Loaded {len(docs)} documents (pages) from text-based PDFs.\")\n",
        "            # Verify sources\n",
        "            sources = set(doc.metadata.get('source', 'Unknown') for doc in docs)\n",
        "            print(\"Sources loaded:\", sources)\n",
        "            print(\"Sample document metadata (first page):\", docs[0].metadata)\n",
        "            # Note: PyPDFDirectoryLoader usually correctly populates 'source' and 'page' metadata.\n",
        "        else:\n",
        "            print(\"No text-based PDF documents were loaded.\")\n",
        "            print(\"Check if the directory contains text-based PDFs or if they are corrupted.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred loading PDFs with PyPDFDirectoryLoader: {e}\")\n",
        "        print(\"Ensure only text-based PDFs are in the directory or that 'pypdf' library is working.\")\n",
        "        docs = [] # Ensure docs is empty on error\n",
        "\n",
        "# Now the 'docs' variable holds pages loaded from text-based PDFs\n",
        "if not docs:\n",
        "    print(\"\\nWarning: No documents were loaded. Subsequent steps might fail.\")"
      ],
      "metadata": {
        "id": "SNbTLHR1DdnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Documents into Chunks\n",
        "splits = []\n",
        "if docs:\n",
        "    print(\"\\n--- Splitting Documents into Chunks ---\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=2000, # Adjust chunk size based on embedding model limits & desired context\n",
        "        chunk_overlap=300  # Overlap helps maintain context between chunks\n",
        "    )\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "    print(f\"Split into {len(splits)} chunks.\")\n",
        "    if splits:\n",
        "        print(\"Sample chunk metadata:\", splits[0].metadata)\n",
        "else:\n",
        "    print(\"\\nSkipping splitting: No documents loaded.\")"
      ],
      "metadata": {
        "id": "KBDfyMAuDghB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Initialize Embedding Model\n",
        "embedding_model = None\n",
        "if GOOGLE_API_KEY:\n",
        "    print(\"\\n--- Initializing Gemini Embedding Model ---\")\n",
        "    try:\n",
        "        # Use a Gemini embedding model available via the API\n",
        "        embedding_model = GoogleGenerativeAIEmbeddings(\n",
        "            model=\"models/text-embedding-004\",\n",
        "            google_api_key=GOOGLE_API_KEY\n",
        "        )\n",
        "        print(f\"Initialized embedding model: {embedding_model.model}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing embedding model: {e}\")\n",
        "else:\n",
        "    print(\"\\nSkipping embedding model initialization: API Key not configured.\")"
      ],
      "metadata": {
        "id": "2OtCdXiPDsuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create or Load Vector Store\n",
        "vectorstore = None\n",
        "db_creation_needed = True # Flag to check if we need to create the DB\n",
        "\n",
        "if embedding_model and splits:\n",
        "    print(f\"\\n--- Setting up Chroma Vector Store ---\")\n",
        "    print(f\"Using persistent path: {chroma_persist_path}\")\n",
        "\n",
        "    # Check if the database directory already exists and isn't empty\n",
        "    if os.path.exists(chroma_persist_path) and os.listdir(chroma_persist_path):\n",
        "        print(\"Existing ChromaDB directory found. Attempting to load...\")\n",
        "        try:\n",
        "            vectorstore = Chroma(\n",
        "                persist_directory=chroma_persist_path,\n",
        "                embedding_function=embedding_model\n",
        "            )\n",
        "            # Quick check to see if it loaded something plausible\n",
        "            test_search = vectorstore.similarity_search(\"insurance\", k=1)\n",
        "            if test_search:\n",
        "                 print(\"Successfully loaded existing vector store.\")\n",
        "                 db_creation_needed = False\n",
        "            else:\n",
        "                 print(\"Loaded directory, but store seems empty or invalid. Will recreate.\")\n",
        "                 # Consider clearing the directory here if needed: shutil.rmtree(chroma_persist_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading existing vector store: {e}. Will attempt to recreate.\")\n",
        "            # Consider clearing the directory here if needed: shutil.rmtree(chroma_persist_path)\n",
        "\n",
        "\n",
        "    if db_creation_needed:\n",
        "        print(\"Creating new Chroma vector store...\")\n",
        "        print(f\"Embedding {len(splits)} chunks. This may take a significant amount of time...\")\n",
        "        try:\n",
        "            # Create Chroma vector store FROM the document splits and WITH the Gemini embedding function\n",
        "            vectorstore = Chroma.from_documents(\n",
        "                documents=splits,\n",
        "                embedding=embedding_model,\n",
        "                persist_directory=chroma_persist_path # Save persistently\n",
        "            )\n",
        "            print(\"New Chroma vector store created and documents embedded.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating new Chroma vector store: {e}\")\n",
        "            vectorstore = None\n",
        "    else:\n",
        "        print(\"Using previously loaded vector store.\")\n",
        "\n",
        "\n",
        "elif not embedding_model:\n",
        "     print(\"\\nSkipping vector store setup: Embedding model not initialized.\")\n",
        "elif not splits:\n",
        "     print(\"\\nSkipping vector store setup: No document splits available.\")"
      ],
      "metadata": {
        "id": "CkHZU7ICDvqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Retriever\n",
        "retriever = None\n",
        "if vectorstore:\n",
        "    print(\"\\n--- Creating Retriever ---\")\n",
        "\n",
        "    base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})  # Fetch more for potential reranking\n",
        "    print(\"Base retriever created (fetches top 20).\")\n",
        "\n",
        "    try:\n",
        "        print(\"Attempting to set up CrossEncoder reranker using LangChain wrapper...\")\n",
        "\n",
        "        from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "\n",
        "        hf_cross_encoder_model = HuggingFaceCrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "        reranker = CrossEncoderReranker(\n",
        "            model=hf_cross_encoder_model,  # Use LangChain wrapper as model\n",
        "            top_n=3                        # Return top 3 after reranking\n",
        "        )\n",
        "        # Use the reranker as a context compression retriever\n",
        "        reranker_retriever = ContextualCompressionRetriever(\n",
        "            base_compressor=reranker, base_retriever=base_retriever\n",
        "        )\n",
        "\n",
        "        retriever = reranker_retriever  # Use the reranking retriever\n",
        "        print(\"Contextual Compression Retriever with reranker created (returns top 3).\")\n",
        "\n",
        "    except ImportError:\n",
        "        # Handle case where LangChain or sentence-transformers might not be fully installed\n",
        "        print(\"ImportError occurred. Ensure 'sentence-transformers' and 'langchain' are installed. Using base retriever instead.\")\n",
        "        retriever = base_retriever\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle other exceptions\n",
        "        print(f\"Could not set up reranker: {e}. Using base retriever instead.\")\n",
        "        retriever = base_retriever  # Fallback to base retriever\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping retriever creation: Vector store not available.\")\n"
      ],
      "metadata": {
        "id": "3cGnJeJXKe_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Retriever (Direct Implementation of Re-ranking)\n",
        "retriever = None # Initialize retriever variable\n",
        "\n",
        "if vectorstore: # Only proceed if the vector store was successfully created/loaded\n",
        "    print(\"\\n--- Creating Retriever with Re-ranking ---\")\n",
        "    try:\n",
        "        # 1. Import the necessary components\n",
        "        from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "        from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "        from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "        # 2. Define the base retriever (fetches initial candidates)\n",
        "        base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20}) # Fetch 20 candidates\n",
        "        print(\"Base retriever created (fetches top 20).\")\n",
        "\n",
        "        # 3. Initialize the LangChain wrapper for the cross-encoder model\n",
        "        print(\"Initializing CrossEncoder model...\")\n",
        "        hf_cross_encoder_model = HuggingFaceCrossEncoder(model_name='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "        # 4. Initialize the reranker component using the wrapper\n",
        "        reranker = CrossEncoderReranker(\n",
        "            model=hf_cross_encoder_model,\n",
        "            top_n=3 # Return the top 3 most relevant documents after reranking\n",
        "        )\n",
        "        print(\"CrossEncoderReranker component created (will return top 3).\")\n",
        "\n",
        "        # 5. Create the Contextual Compression Retriever\n",
        "        # This wraps the base_retriever and uses the reranker to compress/filter results\n",
        "        compression_retriever = ContextualCompressionRetriever(\n",
        "            base_compressor=reranker,\n",
        "            base_retriever=base_retriever\n",
        "        )\n",
        "        retriever = compression_retriever # Assign the final reranking retriever\n",
        "        print(\"Contextual Compression Retriever setup complete.\")\n",
        "\n",
        "    except ImportError as e:\n",
        "         # Handle potential missing libraries\n",
        "         print(f\"ImportError: Could not import necessary components ({e}).\")\n",
        "         print(\"Ensure 'langchain_community' and 'sentence-transformers' are installed.\")\n",
        "         print(\"Retriever setup failed.\")\n",
        "    except Exception as e:\n",
        "        # Catch other potential errors during setup\n",
        "        print(f\"An error occurred during retriever setup: {e}\")\n",
        "        print(\"Retriever setup failed.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping retriever creation: Vector store not available from previous step.\")\n",
        "\n",
        "# Final check\n",
        "if retriever:\n",
        "    print(\"\\nRetriever is ready.\")\n",
        "else:\n",
        "    print(\"\\nRetriever was not successfully created.\")"
      ],
      "metadata": {
        "id": "Vl3MwQtvNwL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LLM\n",
        "llm = None\n",
        "if GOOGLE_API_KEY:\n",
        "    print(\"\\n--- Initializing Gemini LLM ---\")\n",
        "    try:\n",
        "        # Choose a Gemini generative model\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash-latest\",\n",
        "            google_api_key=GOOGLE_API_KEY,\n",
        "            temperature=0.0, # Low temperature for factual answers\n",
        "            convert_system_message_to_human=True\n",
        "        )\n",
        "        print(f\"Initialized LLM: {llm.model}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing LLM: {e}\")\n",
        "else:\n",
        "    print(\"\\nSkipping LLM initialization: API Key not configured.\")"
      ],
      "metadata": {
        "id": "qU-hCCLgDzLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define RAG Chain\n",
        "rag_chain = None\n",
        "if retriever and llm:\n",
        "    print(\"\\n--- Defining RAG Chain ---\")\n",
        "    # Define prompt template\n",
        "    template = \"\"\"\n",
        "You are an intelligent assistant helping users understand insurance policies.\n",
        "\n",
        "Your job is to answer the question strictly based on the provided context.\n",
        "Do not use any prior knowledge or external sources. If the answer is not in the context, reply:\n",
        "\"The answer is not found in the provided documents.\"\n",
        "\n",
        "Instructions:\n",
        "- Only answer using facts present in the context.\n",
        "- Summarize the relevant information clearly and accurately.\n",
        "- When possible, cite the document name and page number like this: [Source: document_name, Page: page_number].\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "    # Function to format retrieved documents\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(f\"Source: {doc.metadata.get('source', 'Unknown')}, Page: {doc.metadata.get('page', 'N/A')}\\nContent: {doc.page_content}\" for doc in docs)\n",
        "\n",
        "    # Define the chain using LangChain Expression Language (LCEL)\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    print(\"RAG chain defined successfully.\")\n",
        "else:\n",
        "    print(\"\\nCannot define RAG chain: Retriever or LLM not available.\")"
      ],
      "metadata": {
        "id": "um2mtk5vD08J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# Run Query\n",
        "if rag_chain:\n",
        "    print(\"\\n--- Ready to Query ---\\n\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_query = input(\"Enter your question (or type 'quit' to exit): \").strip()\n",
        "\n",
        "            if user_query.lower() == 'quit':\n",
        "                print(\"\\nExiting the query session. Goodbye!\")\n",
        "                break\n",
        "            if not user_query:\n",
        "                print(\"\\nPlease enter a valid query.\")\n",
        "                continue\n",
        "\n",
        "            print(\"\\nGenerating response...\\n\")\n",
        "            final_response = rag_chain.invoke(user_query)\n",
        "\n",
        "            print(\"-\"*50)\n",
        "            print(\"                  Final Answer\")\n",
        "            print(\"-\"*50)\n",
        "\n",
        "            # Wrap text to fit the output width (80 characters in this case)\n",
        "            wrapper = textwrap.TextWrapper(width=80, break_long_words=False)\n",
        "\n",
        "            # Prepare summary\n",
        "            summary = []\n",
        "            paragraphs = final_response.split('[Source:')\n",
        "\n",
        "            # Process paragraphs and add source references inline\n",
        "            for i, paragraph in enumerate(paragraphs):\n",
        "                paragraph = paragraph.strip()\n",
        "\n",
        "                if '[Source:' in paragraph:\n",
        "                    # Add the source reference inline with the paragraph\n",
        "                    source_reference = '[Source:' + '[Source:'.join(final_response.split('[Source:')[i + 1:]).split(']')[0] + ']'\n",
        "                    paragraph += \" \" + source_reference\n",
        "                wrapped_paragraph = wrapper.fill(paragraph)\n",
        "                summary.append(wrapped_paragraph)\n",
        "\n",
        "            # Print summary (with sources inline)\n",
        "            print(\"\\n** Summary **\\n\")\n",
        "            print(\"\\n\".join(summary))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred: {e}\")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nExiting the query session. Goodbye!\")\n",
        "            break\n",
        "else:\n",
        "    print(\"\\nCannot run queries: RAG chain was not set up successfully.\")\n"
      ],
      "metadata": {
        "id": "WnZ43h9wID-F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}